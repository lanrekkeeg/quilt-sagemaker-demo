{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25c80864564c1cfedf6236eb4aac0d75c5f45ea8"
   },
   "source": [
    "# Building a CNN on Fashion MNIST\n",
    "\n",
    "This notebook demonstrates how a data scientist would approach exploring a dataset, building a model, and saving that model within a workflow utilizing [Quilt T4](https://quiltdocs.gitbook.io/t4/).\n",
    "\n",
    "We will build a convolutional neural network based on the [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist) dataset. Fashion MNIST is an image dataset curated by the folks at Zalando Research specifically for the purpose of providing a good default dataset for use in image classification tasks. It consists of a moderately large set of 28x28 images of various fashion items, which have been assigned to one of ten different classes. This dataset is useful for benchmarking image classification challenges, as it is significantly harder than the extremely well-known MNIST dataset, whilst still being easy enough that relatively straightforward image classification techniques may be used to solve it. Classification of clothing (for the purposes of recommendation) is a very well-traveled problem in startups these days, and this well-curated dataset is a great introduction to this problem.\n",
    "\n",
    "We'll cover the full workflow typical for a data scientist: exploration, model training, model evaluation, and then saving that model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "The full list of dependencies required to run this notebook is available in the `requirements.txt` file bundled with this code repository. To run this notebook successfully you should first install these: `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51c19e62535867061bfa0999c52563156ecfb810"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "The first step to building a model is actually getting the data. We will use [Quilt T4](https://github.com/quiltdata/t4) to do this.\n",
    "\n",
    "Depending on when you are reading this, an instance of Quilt T4 that you can publicly download from may or may not be ready yet. If it is, great! Run the following code cell and you will be good to go.\n",
    "\n",
    "If it isn't, you can also get the data directly from [its source repository](https://github.com/zalandoresearch/fashion-mnist) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lanrekkeeg/quilt-sagemaker-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('quilt-sagemaker-demo/fashion_mnist-train.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8432c8b5adcafb69356f662f99bcde31fbe1fc2"
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "There are 10 unique classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3432c2c69335bf7196dd7282a49366ef38860285"
   },
   "outputs": [],
   "source": [
    "sorted(data_train.label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot these out we can see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24cf989c48a36249503648ee678008dfd751c7d8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "labels = sorted(data_train.label.unique())\n",
    "n_samples = 10\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axarr = plt.subplots(len(labels), n_samples, figsize=(16, 8))\n",
    "\n",
    "for j, label in enumerate(labels):\n",
    "    for k, img_arr in enumerate(data_train.query(\"label == @label\").sample(n_samples, random_state=42)\\\n",
    "                                    .values[:, 1:].reshape((n_samples, 28, 28))):\n",
    "        axarr[j][k].imshow(img_arr)\n",
    "\n",
    "fig.suptitle(\"Fashion-MNIST Images Classes\", fontsize=20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can back this out into explicit class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3639ee6b8e46328596ca892a1cf5a8bfd3bf350"
   },
   "outputs": [],
   "source": [
    "img_class_key = {\n",
    "    0: 't-shirt',\n",
    "    1: 'pants',\n",
    "    2: 'pullover',\n",
    "    3: 'dress',\n",
    "    4: 'coat',\n",
    "    5: 'sandal',\n",
    "    6: 'shirt',\n",
    "    7: 'sneakers',\n",
    "    8: 'handbag',\n",
    "    9: 'boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is completely balanced: every class appears in the dataset the same number of times, exactly 6000. This is great because it prevents using from running into any problems with having to perform [imbalanced learning](https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bf2ab998e95fe2319d12cba4bf1a3b0a91e3e44"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "label_counts = data_train['label'].value_counts()\n",
    "label_counts.index = img_class_key.values()\n",
    "label_counts.plot.bar(color='steelblue', figsize=(8, 4))\n",
    "sns.despine()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a30cf9fa5348ffcfbf36fd874b4a99dd08bea482"
   },
   "source": [
    "## Training a model\n",
    "\n",
    "For the purposes of demonstration, let's train a CNN model on this dataset. CNNs provide the current bleeding edge of performance on these kinds of image classification problems. If you are unfamiliar with CNNs, [this blog post](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/) provides a good conceptual overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# exclude the class label from the training data, otherwise we have nothing to train on\n",
    "X = data_train.iloc[:, 1:].values\n",
    "\n",
    "# one-hot encode the classes\n",
    "y = pd.get_dummies(data_train.iloc[:, 0].values).values\n",
    "\n",
    "# partition the dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the flat [0, 255]-entry list into a [0, 1]-entry grid, as desired by the CNN.\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aee85ca3b168c8aad8969da59d0e64663ca626ae"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "clf = Sequential()\n",
    "clf.add(Conv2D(32, kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               input_shape=(28, 28, 1)))\n",
    "clf.add(MaxPooling2D((2, 2)))\n",
    "clf.add(Dropout(0.25))\n",
    "clf.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "clf.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "clf.add(Dropout(0.25))\n",
    "clf.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "clf.add(Dropout(0.4))\n",
    "clf.add(Flatten())\n",
    "clf.add(Dense(128, activation='relu'))\n",
    "clf.add(Dropout(0.3))\n",
    "clf.add(Dense(10, activation='softmax'))\n",
    "\n",
    "clf.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**\n",
    "\n",
    "For educational purposes we have set the number of epochs in the following code cell to just 1.\n",
    "\n",
    "This will fail to generate a robust machine learning model but will make training take almost no time at all, which makes standing up this model yourself much easier.\n",
    "\n",
    "To reproduce the robust outcomes in the evaluation section of this notebook set `epochs` to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00bddf187494dea70ae7af966c978fa2b537e76e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = clf.fit(X_train, y_train, batch_size=512, epochs=1, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e631fdbad208f4d8dfc3987401ebed46c8c3eac"
   },
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate model performance, let's start by looking at the training accuracy over training epochs. If we did a good job training our model the training and validation accuracy and loss will both be similar monotonic curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69b3b968710ab005151231507b58d8c5e53d8a28",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "accuracy = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
    "axarr[0].plot(epochs, accuracy)\n",
    "axarr[0].set_title('Training accuracy')\n",
    "sns.despine()\n",
    "\n",
    "axarr[1].plot(epochs, loss)\n",
    "axarr[1].set_title('Training loss')\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy = history.history['val_acc']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(val_accuracy))\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
    "axarr[0].plot(epochs, val_accuracy)\n",
    "axarr[0].set_title('Validation accuracy')\n",
    "sns.despine()\n",
    "\n",
    "axarr[1].plot(epochs, val_loss)\n",
    "axarr[1].set_title('Validation loss')\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Let's see what classes the classifier performed well on and which ones it struggled with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49c7f75312dd31f97d4b45b21c950d16eb70f2ca"
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict_classes(X_test)\n",
    "y_test_classed = np.nonzero(y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc7727bc85e6c6628911901b29354908819f928a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_pred, y_test_classed, target_names=img_class_key.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b542f450f907a198c883bfd3abb687e51dd44d4"
   },
   "source": [
    "We see here that certain classes are noticeably more distinguishable than others. We scored allmost all pants, sandals, and handbags correctly, indicating that these classes are highly separable. We fared worst on upper body clothing items: pullovers, coats, and especially shirts. None of this is very surprising; the silhouettes of shoes are very easy to tell apart from those of shirts, but amongst the different classes of shirts there's a lot of potential overlap.\n",
    "\n",
    "Finally to evaluate our model complexity we can inspect which areas of the images the CNN is firing on. The following code snippet courtesy of Fran√ßois Chollet, the original author of Keras, who provides it in his \"Deep Learning with Python\" book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = X_train[154]\n",
    "plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98936a5c3b92b1f7b1ec2dbe65b99f598596d8ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "layer_outputs = [layer.output for layer in clf.layers[:8]]\n",
    "activation_model = models.Model(input=clf.input, output=layer_outputs)\n",
    "activations = activation_model.predict(test_im.reshape(1,28,28,1))\n",
    "\n",
    "layer_names = []\n",
    "for layer in clf.layers[:-1]:\n",
    "    layer_names.append(layer.name) \n",
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    if layer_name.startswith('conv'):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,:, :, col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN works by aggregating the features of an image into progressively higher feature pools, so the earlier layers in this visualization are easier to interpret than the later ones. We see that the first layer seems to fire mostly on edges of the image, and on the distinction between foreground and background in the image. The second layer seems to begin to focus on patterns on the article of clothing itself. The third layer is difficult to interpret.\n",
    "\n",
    "Note that some images are completely blacked out, indicating neurons that did not fire at all. This is because those neurons were deadened by the regularization properties of the ReLU activation layers we are using in this CNN. A significant but not overwhelming percentage of the neurons in our third convolutional layer never fire, indicating relatively good fit of our model architecture to this data: too simple and no neurons will be regulated out; too complex and many more will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our CNN in hand we finish by saving the model artifact. Note that you need to have access to the `s3://quilt-example` S3 bucket for the following code cell to work; if you do not have access to that bucket you should replace it in the path below with one you do have access to. `keras` models have a `save` command that we can run to persist a model to disk as an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save('clf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker API expects model artifacts to be written to `/opt/ml/model`. It will zip that directory up into a `*.tar.gz` file and serialize that to an S3 filepath of your choice.\n",
    "\n",
    "So to make this model-building notebook compatible wih SageMaker, we may do one of two things:\n",
    "\n",
    "* Use `tar` to generate an archive file from the raw HDF5, and send that to S3. Then, every time we want to launch this model in AWS SageMaker, we will will have to first overwrite the estimator `model_path` like so:\n",
    "\n",
    "    ```ipython\n",
    "    sage.estimator.Estimator.model_data =\\\n",
    "    \"s3://alpha-quilt-storage/aleksey/fashion_mnist_clf/clf.tar.gz\"\n",
    "    ```\n",
    "\n",
    "* The alternative would be to save the raw `clf.h5` to a human-readable data package, and then additionally save the file to `/opts/ml/clf.h5` and letting Amazon archive and upload that to S3 for us. This doesn't require futzing with the `Estimator` class later on when we are trying to use the model.\n",
    "\n",
    "I ultimately opted for the former option. Since the classifier is small, we can just create a single data package with both the raw `clf.h5` file and the SageMaker-compatible `clf.tar.gz` file; but for larger classifiers it'd be more elegant to keep them in separate packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import t4\n",
    "#(t4.Package()\n",
    "#     .set(\"clf.h5\", \"clf.h5\")\n",
    "#     .set(\"clf.tar.gz\", \"clf.tar.gz\")\n",
    "#     .push(\"quilt/quilt_sagemaker_demo\", \"s3://faisal-sagamaker\", registry=\"s3://faisal-sagamaker\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the model to `/opt/ml/model` so that SageMaker sees it.\n",
    "\n",
    "**User Note**: Do not run the following line of code locally! This will create a file in the `/opt/` folder in your home directory which you don't want. This line is only here for use in a SageMaker-compatible Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf clf.tar.gz clf.h5\n",
    "!cp clf.tar.gz /opt/ml/model/clf.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
